import os
import pandas as pd
import pickle
import numpy as np
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix

def run_inference(model, data_path, dataset_name, output_dir, master_label_df=None):
    """
    é€šç”¨æ¨ç†å‡½æ•° (æ”¯æŒé€šè¿‡IDåŒ¹é…å¤–éƒ¨æ ‡ç­¾)
    :param model: å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡
    :param data_path: æ•°æ®CSVè·¯å¾„
    :param dataset_name: æ•°æ®é›†åç§°
    :param output_dir: ç»“æœä¿å­˜ç›®å½•
    :param master_label_df: åŒ…å«æ‰€æœ‰IDå’ŒLabelçš„æ€»è¡¨ DataFrame (å¯é€‰)
    """
    print(f"\n{'='*20} æ­£åœ¨å¤„ç†: {dataset_name} {'='*20}")
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {data_path}ï¼Œè·³è¿‡è¯¥æ•°æ®é›†ã€‚")
        return None

    # 1. åŠ è½½ç‰¹å¾æ•°æ®
    df = pd.read_csv(data_path)
    
    # === å…³é”®ä¿®æ”¹ï¼šç»Ÿä¸€ ID æ ¼å¼ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢åŒ¹é…å¤±è´¥ ===
    if 'ID' in df.columns:
        df['ID'] = df['ID'].astype(str)
    else:
        print("âš ï¸ è­¦å‘Š: æ•°æ®ä¸­æ²¡æœ‰ 'ID' åˆ—ï¼Œæ— æ³•åŒ¹é…æ ‡ç­¾ï¼Œå°†ä»…è¿›è¡Œé¢„æµ‹ã€‚")
        ids = range(len(df)) # ç”Ÿæˆé»˜è®¤ç´¢å¼•
        # å¦‚æœæ²¡æœ‰IDåˆ—ï¼Œæ— æ³•è¿›è¡Œ mergeï¼Œåªèƒ½ç”¨ df è‡ªèº«
        master_label_df = None 

    # 2. åŒ¹é…æ ‡ç­¾ (Label Matching)
    y_true = None
    
    # å¦‚æœæä¾›äº†æ€»æ ‡ç­¾è¡¨ï¼Œå¹¶ä¸”æ•°æ®ä¸­æœ‰IDï¼Œåˆ™è¿›è¡Œåˆå¹¶
    if master_label_df is not None and 'ID' in df.columns:
        print("æ­£åœ¨é€šè¿‡ ID åŒ¹é…æ ‡ç­¾...")
        
        # å…ˆåˆ é™¤ç‰¹å¾æ–‡ä»¶ä¸­å¯èƒ½å­˜åœ¨çš„æ—§ label åˆ—ï¼Œé¿å… merge äº§ç”Ÿ label_x, label_y
        if 'label' in df.columns:
            df = df.drop(columns=['label'])
            
        # Left Join: ä¿ç•™ç‰¹å¾æ–‡ä»¶çš„æ‰€æœ‰è¡Œï¼ŒåŒ¹é…ä¸Šçš„å¡«å…¥æ ‡ç­¾ï¼Œæ²¡åŒ¹é…ä¸Šçš„ä¸º NaN
        df_merged = pd.merge(df, master_label_df[['ID', 'label']], on='ID', how='left')
        
        # æå–æ ‡ç­¾
        if 'label' in df_merged.columns:
            y_true = df_merged['label'].values
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…å¤±è´¥çš„æƒ…å†µ
            nan_count = df_merged['label'].isna().sum()
            if nan_count > 0:
                print(f"âš ï¸ æ³¨æ„: æœ‰ {nan_count} ä¸ªæ ·æœ¬æœªåœ¨ label.csv ä¸­æ‰¾åˆ°å¯¹åº”çš„ ID (Label ä¸º NaN)ã€‚")
        
        # æ›´æ–°ç”¨äºé¢„æµ‹çš„ df (æ­¤æ—¶ df_merged åŒ…å«ç‰¹å¾ + ID + label)
        # æˆ‘ä»¬éœ€è¦æŠŠ ID å’Œ label æ‹¿èµ°ï¼Œåªç•™ç‰¹å¾
        ids = df_merged['ID']
        data_for_pred = df_merged.drop(columns=['ID', 'label'])
        
    else:
        # å¦‚æœæ²¡æœ‰æä¾›æ€»è¡¨ï¼Œå°è¯•ç›´æ¥ä»æ–‡ä»¶è¯»å– label
        if 'ID' in df.columns:
            ids = df['ID']
            data_for_pred = df.drop(columns=['ID'])
        else:
            ids = range(len(df))
            data_for_pred = df.copy()
            
        if 'label' in data_for_pred.columns:
            y_true = data_for_pred['label'].values
            data_for_pred = data_for_pred.drop(columns=['label'])

    # 3. ç‰¹å¾å¯¹é½ (è‡ªåŠ¨ç­›é€‰é€»è¾‘)
    expected_features = getattr(model, "n_features_in_", None)
    
    if expected_features and data_for_pred.shape[1] > expected_features:
        print(f"â„¹ï¸ æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾æ•° ({data_for_pred.shape[1]}) å¤šäºæ¨¡å‹éœ€æ±‚ ({expected_features})")
        print("å°è¯•åŠ è½½ 'train_selected_features_lasso.csv' è·å–ç‰¹å¾åç§°åˆ—è¡¨ä»¥è¿›è¡Œç­›é€‰...")
        
        # å¯»æ‰¾å‚è€ƒæ–‡ä»¶
        feature_ref_path = os.path.join(os.path.dirname(data_path), 'train_selected_features_lasso.csv')
        # å¦‚æœå½“å‰ç›®å½•ä¸‹æ‰¾ä¸åˆ°ï¼Œä¹Ÿå¯ä»¥å°è¯•å†™æ­»ä¸€ä¸ªè·¯å¾„æˆ–è€…ä» data_path æ¨å¯¼
        if not os.path.exists(feature_ref_path):
             # å°è¯•åœ¨ä¸Šçº§ç›®å½•æ‰¾ (æ ¹æ®ä½ çš„æ–‡ä»¶ç»“æ„å¯èƒ½éœ€è¦è°ƒæ•´)
             feature_ref_path = r"F:\new_yq\data\yq5mm\exval_data\img\train_selected_features_lasso.csv"

        if os.path.exists(feature_ref_path):
            df_ref = pd.read_csv(feature_ref_path)
            feature_names = [c for c in df_ref.columns if c not in ['ID', 'label']]
            
            missing_cols = [c for c in feature_names if c not in data_for_pred.columns]
            if not missing_cols:
                print(f"âœ… æˆåŠŸåŒ¹é…ç‰¹å¾åˆ—è¡¨ï¼Œç­›é€‰å‡º {len(feature_names)} ä¸ªç‰¹å¾ã€‚")
                data_for_pred = data_for_pred[feature_names]
            else:
                print(f"âŒ é”™è¯¯: ç¼ºå¤±ç‰¹å¾: {missing_cols}")
                return None
        else:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°ç‰¹å¾å‚è€ƒæ–‡ä»¶: {feature_ref_path}")
    
    # 4. é¢„æµ‹
    try:
        pred_labels = model.predict(data_for_pred)
        
        if hasattr(model, "predict_proba"):
            pred_probs = model.predict_proba(data_for_pred)[:, 1]
        else:
            d_vals = model.decision_function(data_for_pred)
            pred_probs = (d_vals - d_vals.min()) / (d_vals.max() - d_vals.min())
            
    except Exception as e:
        print(f"âŒ é¢„æµ‹é”™è¯¯: {e}")
        return None

    # 5. è®¡ç®—æŒ‡æ ‡ & è¾“å‡º
    auc_score = "N/A"
    
    if y_true is not None:
        # âš ï¸ å…³é”®ï¼šè®¡ç®—æŒ‡æ ‡æ—¶è¦ç§»é™¤ NaN çš„ Label (æœªåŒ¹é…åˆ°çš„æ•°æ®)
        valid_mask = ~pd.isna(y_true)
        if np.sum(valid_mask) > 0:
            # ä»…åœ¨æœ‰æ ‡ç­¾çš„æ•°æ®ä¸Šè®¡ç®— AUC
            y_true_valid = y_true[valid_mask]
            pred_probs_valid = pred_probs[valid_mask]
            pred_labels_valid = pred_labels[valid_mask]
            
            try:
                auc_score = roc_auc_score(y_true_valid, pred_probs_valid)
                acc_score = accuracy_score(y_true_valid, pred_labels_valid)
                print(f"ğŸ“Š {dataset_name} æŒ‡æ ‡ (åŸºäº {np.sum(valid_mask)} ä¸ªåŒ¹é…æ ·æœ¬):")
                print(f"   AUC:      {auc_score:.4f}")
                print(f"   Accuracy: {acc_score:.4f}")
            except ValueError as e:
                print(f"âš ï¸ æ— æ³•è®¡ç®—æŒ‡æ ‡ (å¯èƒ½æ˜¯æ ‡ç­¾åªæœ‰ä¸€ä¸ªç±»åˆ«): {e}")
        else:
            print("âš ï¸ æ‰€æœ‰æ ·æœ¬å‡æœªåŒ¹é…åˆ°æ ‡ç­¾ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚")

    # 6. ä¿å­˜ç»“æœ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    res_df = pd.DataFrame()
    res_df['ID'] = ids
    if y_true is not None:
        res_df['True_Label'] = y_true
    res_df['SVM_Predicted'] = pred_labels
    res_df['SVM_Probability'] = pred_probs
    
    save_path = os.path.join(output_dir, f'inference_result_{dataset_name}.csv')
    res_df.to_csv(save_path, index=False)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {save_path}")
    
    return auc_score


if __name__ == "__main__":
    # ================= é…ç½®åŒºåŸŸ =================
    
    model_path = r"model\in\SVM.pkl"
    # æ€» Label è¡¨çš„è·¯å¾„
    label_path = r"data\label.csv"
    
    output_directory = r"result\in"
    
    datasets_to_process = {
        # ä½ çš„æ•°æ®é›†è·¯å¾„
        "train": r"data\in\train_selected_features_lasso.csv",
        "test": r"data\in\train_selected_features_lasso.csv",
        # "ExVal_1": r"F:\new_yq\data\yq5mm\exval_data1\step8.0_RadiomicsCombind\deleteNaN.csv",
    }
    
    # ================= æ‰§è¡ŒåŒºåŸŸ =================
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    if os.path.exists(model_path):
        with open(model_path, 'rb') as f:
            svm_model = pickle.load(f)
    else:
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼")
        exit()

    # 2. === æ–°å¢æ­¥éª¤ï¼šåŠ è½½æ€» Label è¡¨ ===
    print(f"æ­£åœ¨åŠ è½½ Label è¡¨: {label_path}")
    master_label_df = None
    if os.path.exists(label_path):
        master_label_df = pd.read_csv(label_path)
        # å¼ºåˆ¶å°† ID è½¬ä¸º stringï¼Œç¡®ä¿ä¸ç‰¹å¾è¡¨ä¸­çš„ ID ç±»å‹ä¸€è‡´ï¼Œå¦åˆ™ merge ä¼šå¤±è´¥
        if 'ID' in master_label_df.columns:
            master_label_df['ID'] = master_label_df['ID'].astype(str)
            print(f"Label è¡¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(master_label_df)} ä¸ªæ ·æœ¬ã€‚")
        else:
            print("âŒ é”™è¯¯ï¼šLabel è¡¨ä¸­æ‰¾ä¸åˆ° 'ID' åˆ—ï¼")
            exit()
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° Label è¡¨æ–‡ä»¶ï¼Œåç»­å°†æ— æ³•è®¡ç®— AUC æŒ‡æ ‡ã€‚")

    # 3. å¾ªç¯å¤„ç†
    summary_results = []
    
    for name, path in datasets_to_process.items():
        if not path or not os.path.exists(path):
            print(f"è·³è¿‡ {name}: è·¯å¾„ä¸å­˜åœ¨")
            continue
            
        # å°† master_label_df ä¼ é€’ç»™å‡½æ•°
        auc = run_inference(svm_model, path, name, output_directory, master_label_df)
        summary_results.append({"Dataset": name, "AUC": auc})

    # 4. æ±‡æ€»
    print("\n" + "="*30)
    print("       æœ€ç»ˆç»“æœæ±‡æ€»       ")
    print("="*30)
    df_summary = pd.DataFrame(summary_results)
    print(df_summary)
    
    if not df_summary.empty:
        df_summary.to_csv(os.path.join(output_directory, "summary_auc.csv"), index=False)